{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correos electrónicos SPAM: Un enfoque con Procesamiento de Lenguaje Natural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alumno: Edgar Adian Chávez Medina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docente: Ing. Samuel Martin Martinez Magdaleno "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los correos electrónicos no deseados en su bandeja son molestos ya que peturban la rutina del usuario. Es por eso que las cuentas de correo electrónico ya que tiene un filtro spam. Dado que es una de las aplicaciones de PLN mas utilizadas vamos a ver cómo se desarrollo un filtro de spam simple para correos electrónicos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las famosas librerias \n",
    "from functools import reduce \n",
    "\n",
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd \n",
    "import string \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insertando los datos\n",
    "full_corpus = pd.read_csv('SMSSpamCollection.tsv',sep='\\t', header=None, names=['label', 'msg_body'])\n",
    "\n",
    "#Separamso los mensajes en 'ham' y 'spam'\n",
    "ham_text=[]\n",
    "spam_text=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los N-gramso se usan para modelar el lenguaje en función de la predicción de las palabras, es decir, predice la siguiente palabra de una oración de la palabras N-1 anteriores. Bigram es la secuencia de 2 palabras de N-gramos que predice la siguiente palabra de una oración usando la palabra anterior. En lugar de considerar la historia completa de una oración o una secuencia de palabras en particular, un modelo como bigram puede se ocupado en términos de una aproximación de la historia al ocupar una historia limitada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La identificación de un mensaje como \"ham\" o \"spam\" es una tarea de clasificación ya que la variable de destino tiene valores discretos que son \"ham\" o \"spam\". En esta practica, se usa el modelo bigram, aunque existen muchas técincas avanzadas que se pueden utilizar para este propósito. Para utilizar el modelo bigram para asignar un mensaje dado como \"spam\" o \"ham\", hay varios pasos que deben lograrse:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Inspección y separación de mensajes en las categorías \"ham\" y \"sapam\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente, el conjunto de datos debe inspeccionarse para ocuperlo y abordarlo para lograr la tarea. El formato de los dados, la cantidad de datos proporcionados, la naturaleza de los datos se incluyen en esta inspección para identificar la mejor aproximación posible para la tarea.\n",
    "\n",
    "El corpus de mensaje dado ha marcasdo cada mensaje como ham o spam. Además, hay 5568 mensajes en un DataFrame escrito en inglés que no son objetos nulo. Por lo tanto, el archivo tsv se puede leer usando DataFrame en python para clasificar esos mensajes de acuerdo con el indicador dado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_msgs():\n",
    "    for index, column in full_corpus.iterrows():\n",
    "        label= column[0]\n",
    "        message_text = column[1]\n",
    "        if label == 'ham':\n",
    "            ham_text.append(message_text)\n",
    "        elif label == 'spam':\n",
    "            spam_text.append(message_text)\n",
    "            \n",
    "separate_msgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocesamiento de texto  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El procesamiento es la tarea de realizar los pasos de preparación en el corpus de texto sin formato para completar de manera eficiente una extracción de texto o procesamiento de lenguaje natural o cualquier otra tarea que incluya texto sin formato. el prepocesamiento de texto consta de varios pasos, aunque algunos de ellos pueden no aplicarse a una tarea en particular debido a la naturaleza del conjunto de datos disponibles.\n",
    "\n",
    "En esta tarea, el preprocesamiento de texto incluye los siguientes pasos de acuerdo con el conjunto de datos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminación de signos de puntuación "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminación de los signos de participación de los mensajes de correo electónico\n",
    "def remove_msg_punctuations(email_msg):\n",
    "    punctuation_remove_msg = \"\".join([word for word in email_msg if word not in string.punctuation])\n",
    "    return punctuation_remove_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertir a minúsculas: La conversión de todos los caracteres del texto en un contexto común, como los soportes en minúsculas, impide identificar dos palabras de manera diferente donde una está en minúsculas y la otra no. Por ejemplo, \"Primero\" y \"primero\" deben identificarse como iguales, por lo tanto, poner en minúsculas todos los caracteres facilita la tarea. Además, las palabras de detención también están en minúsculas, por lo que esto también haría posible eliminar palabras de detención más adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenzing: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenzing: La tokenzación es la tarea de dividir el texto en partes significativas, es decir, tokens que incluye oraciones y palabras. Un tokens se puede considerar como una instancia de una secuencia de caracteres en un texto particular que se agrupan para proporcionar una unidad semántica útil para su posterior procesamiento. En esta tarea, la tokenización de palabras se realizan combinando espacios en blanco entre palabras como delimitar. Esto se logra en Python usando expresiones regulares para dividir una cadena en subcadenas con la función split(), que es un tokenizador básico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertir el texto en minusculas y tokenizing de palabras \n",
    "def tokenize_into_word(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palabras lematizantes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La derivaciín es el proceso de eliminar afijos (sufijosm prefijos, infijos, circunfijos) de una palabra para obtener su raíz de la palabra. Aunque la lematización está relacionada con la derivación, difiere ya que la lematización puede capturar formas canónicas basadas en el lema de una palabra. La lematización ocupa vocabulario y un análisi morfológico de la spalabras que lo hacen más rápido y preciso que la derivación. WordNetLemmatizer ha logrado la lematizacón en lenguaje Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatizing \n",
    "word_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatization(tokenized_words):\n",
    "    lemmatized_text = [word_lemmatizer.lemmatize(word)for word in tokenized_word]\n",
    "    return ' '.join(lemmatized_text)\n",
    "\n",
    "def preprocessing_msg(corpus):\n",
    "    categorized_text =pd.DataFrame(corpus)\n",
    "    categorized_text['non_punc_message_body']= categorized_text[0].apply(lambda msg: remove_msg_punctuation(msg))\n",
    "    categorized_text['tokenized_msg_body'] = categorized_text['non_punc_message_body'].apply(lambda msg: tokenize_into_words(msg.lower()))\n",
    "    categorized_text['lemmatized_msg_words'] = categorize_text['tokenized_msg_body'].apply(lambda word_list: lemmatization(word_list))\n",
    "    return categorized_text['lemmatized_msg_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
