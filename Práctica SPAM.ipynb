{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correos electrónicos SPAM: Un enfoque con Procesamiento de Lenguaje Natural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alumno: Edgar Adian Chávez Medina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docente: Ing. Samuel Martin Martinez Magdaleno "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los correos electrónicos no deseados en su bandeja son molestos ya que peturban la rutina del usuario. Es por eso que las cuentas de correo electrónico ya que tiene un filtro spam. Dado que es una de las aplicaciones de PLN mas utilizadas vamos a ver cómo se desarrollo un filtro de spam simple para correos electrónicos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las famosas librerias \n",
    "from functools import reduce \n",
    "\n",
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd \n",
    "import string \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insertando los datos\n",
    "full_corpus = pd.read_csv('SMSSpamCollection.tsv',sep='\\t', header=None, names=['label', 'msg_body'])\n",
    "\n",
    "#Separamso los mensajes en 'ham' y 'spam'\n",
    "ham_text=[]\n",
    "spam_text=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los N-gramso se usan para modelar el lenguaje en función de la predicción de las palabras, es decir, predice la siguiente palabra de una oración de la palabras N-1 anteriores. Bigram es la secuencia de 2 palabras de N-gramos que predice la siguiente palabra de una oración usando la palabra anterior. En lugar de considerar la historia completa de una oración o una secuencia de palabras en particular, un modelo como bigram puede se ocupado en términos de una aproximación de la historia al ocupar una historia limitada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La identificación de un mensaje como \"ham\" o \"spam\" es una tarea de clasificación ya que la variable de destino tiene valores discretos que son \"ham\" o \"spam\". En esta practica, se usa el modelo bigram, aunque existen muchas técincas avanzadas que se pueden utilizar para este propósito. Para utilizar el modelo bigram para asignar un mensaje dado como \"spam\" o \"ham\", hay varios pasos que deben lograrse:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Inspección y separación de mensajes en las categorías \"ham\" y \"sapam\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente, el conjunto de datos debe inspeccionarse para ocuperlo y abordarlo para lograr la tarea. El formato de los dados, la cantidad de datos proporcionados, la naturaleza de los datos se incluyen en esta inspección para identificar la mejor aproximación posible para la tarea.\n",
    "\n",
    "El corpus de mensaje dado ha marcasdo cada mensaje como ham o spam. Además, hay 5568 mensajes en un DataFrame escrito en inglés que no son objetos nulo. Por lo tanto, el archivo tsv se puede leer usando DataFrame en python para clasificar esos mensajes de acuerdo con el indicador dado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_msgs():\n",
    "    for index, column in full_corpus.iterrows():\n",
    "        label= column[0]\n",
    "        message_text = column[1]\n",
    "        if label == 'ham':\n",
    "            ham_text.append(message_text)\n",
    "        elif label == 'spam':\n",
    "            spam_text.append(message_text)\n",
    "            \n",
    "separate_msgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocesamiento de texto  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El procesamiento es la tarea de realizar los pasos de preparación en el corpus de texto sin formato para completar de manera eficiente una extracción de texto o procesamiento de lenguaje natural o cualquier otra tarea que incluya texto sin formato. el prepocesamiento de texto consta de varios pasos, aunque algunos de ellos pueden no aplicarse a una tarea en particular debido a la naturaleza del conjunto de datos disponibles.\n",
    "\n",
    "En esta tarea, el preprocesamiento de texto incluye los siguientes pasos de acuerdo con el conjunto de datos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminación de signos de puntuación "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminación de los signos de participación de los mensajes de correo electónico\n",
    "def remove_msg_punctuations(email_msg):\n",
    "    punctuation_remove_msg = \"\".join([word for word in email_msg if word not in string.punctuation])\n",
    "    return punctuation_remove_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertir a minúsculas: La conversión de todos los caracteres del texto en un contexto común, como los soportes en minúsculas, impide identificar dos palabras de manera diferente donde una está en minúsculas y la otra no. Por ejemplo, \"Primero\" y \"primero\" deben identificarse como iguales, por lo tanto, poner en minúsculas todos los caracteres facilita la tarea. Además, las palabras de detención también están en minúsculas, por lo que esto también haría posible eliminar palabras de detención más adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenzing: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenzing: La tokenzación es la tarea de dividir el texto en partes significativas, es decir, tokens que incluye oraciones y palabras. Un tokens se puede considerar como una instancia de una secuencia de caracteres en un texto particular que se agrupan para proporcionar una unidad semántica útil para su posterior procesamiento. En esta tarea, la tokenización de palabras se realizan combinando espacios en blanco entre palabras como delimitar. Esto se logra en Python usando expresiones regulares para dividir una cadena en subcadenas con la función split(), que es un tokenizador básico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertir el texto en minusculas y tokenizing de palabras \n",
    "def tokenize_into_word(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palabras lematizantes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La derivaciín es el proceso de eliminar afijos (sufijosm prefijos, infijos, circunfijos) de una palabra para obtener su raíz de la palabra. Aunque la lematización está relacionada con la derivación, difiere ya que la lematización puede capturar formas canónicas basadas en el lema de una palabra. La lematización ocupa vocabulario y un análisi morfológico de la spalabras que lo hacen más rápido y preciso que la derivación. WordNetLemmatizer ha logrado la lematizacón en lenguaje Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatizing \n",
    "word_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatization(tokenized_words):\n",
    "    lemmatized_text = [word_lemmatizer.lemmatize(word)for word in tokenized_word]\n",
    "    return ' '.join(lemmatized_text)\n",
    "\n",
    "def preprocessing_msg(corpus):\n",
    "    categorized_text =pd.DataFrame(corpus)\n",
    "    categorized_text['non_punc_message_body']= categorized_text[0].apply(lambda msg: remove_msg_punctuation(msg))\n",
    "    categorized_text['tokenized_msg_body'] = categorized_text['non_punc_message_body'].apply(lambda msg: tokenize_into_words(msg.lower()))\n",
    "    categorized_text['lemmatized_msg_words'] = categorize_text['tokenized_msg_body'].apply(lambda word_list: lemmatization(word_list))\n",
    "    return categorized_text['lemmatized_msg_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extracción de características  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de la etapa de procesamiento, las características deben extraerse del texto. Las características son las unidades que admiten la tarea de clasificación, y las bigrams son las características en esta tarea de clasificación de mensaje. Los bigrams o las características se extran del texto prepocesado. Inicialmente, los unigrams se adquieren, y luego esos unigrams se utilizan para obtener los unigramas en cada rpus (\"ham\" y \"spam\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exxtracción de características. Ejemplo: n-grams\n",
    "def feature_extraction(preprocessed_text):\n",
    "    bigrams = []\n",
    "    unigrams_lists = []\n",
    "    for msg in preprovcessed_text:\n",
    "        #agregamos end of and start of al mensaje\n",
    "        msg ='<s>' + msg + '</s>'\n",
    "        unigrams_lists.append(msg.split())\n",
    "    unigrams = [uni_list for sub_list in unigrams_lists for uni_list in sub_list]\n",
    "    bigrams.extend(nltk.bigrams(unigrams))\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Eliminación de Stop Words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay ciertas palabras en un idioma (se utilizan inglés en la práctica) que son necesarias para una oración o una secuencia de palabras, aunque no contribuyen al significado de una frase considerada. La biblioteca de Kit de herramientas de lenguaje natural (NLTK) en Python proporciona palabras de detención comunes para algunos idiomas\n",
    "\n",
    "En lugar de eliminar las palabras de detención en el paso de preprocesamiento, se realiza después de extraer las características corpus para evitar la ausencia de bigrams con palabras de una parada (('use', 'you'), ('to', 'win')) al adquirir las funciones, ya que tiene un impacto en el resultado final de la aplicación. Las palabras de detención se pueden ignorar en esta Recuperación de información orientada a palabras clave debido a su efecto en la precisión de la recuperación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminando bigrams solo con stop words \n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "def filter_bigrams(bigram_list):\n",
    "    filtered_bigrams = []\n",
    "    for bigram in bigram_list:\n",
    "        if bigram[0] in stopwords and bigram[1] in stopwords:\n",
    "            continue\n",
    "        filteres_bigrams.append(bigram)\n",
    "    return filtered_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Obtener distribución de frecuencia de características  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La distribución de frecuencia se utiliza para obtener la frecuencia de aparición de cada elemento de vocabulario en un texto determinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adquiriendo la frecuencia de caracteristicas\n",
    "def ham_bigram_feature_frequency():\n",
    "    # Frecuencia de caracteristicas para mensajes ham\n",
    "    ham_bigrams = feature_extraction(preprocessing_msgs(ham_text))\n",
    "    ham_bigram_frequency = nltk.FreqDist(filter_stopwords_bigrams(ham_bigrams))\n",
    "    return ham_bigram_frequency\n",
    "\n",
    "def spam_bigram_feature_frequency():\n",
    "    # Frecuencia de caracteristicas para mensaje spam\n",
    "    spam_bigrams = feature_extraction(preprocessing_msgs(spam_text))\n",
    "    spam_bigram_frequency = ntlk.FreqDist(filter_stopwords_bigrams(spam_bigrams))\n",
    "    return spam_bigram_frequency "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construyendo un modelo para la predicción "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo para clasificar un mensaje dado como \"ham\" o \"spam\" se ha abordado calculado las probabilidad de bigram dentro de cada corpus.\n",
    " Inicialmente, el mensaje dado debe procesarse previamente para avanzar con la clasigficaión, incluida la eliminación de signos de puntuación, el cambio de todos los caracteres a minúsclas, la tokenización y la lematización. Luego, los bigrams se extran del texto preprocesado para calcular finalmente la probabilidad de que el texto esté en cada corpus \"ham\" o \"spam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessing_msgs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-c9fd4ff4cad7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\\"'\u001b[0m \u001b[1;33m+\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'\\\" es un mensaje Spam'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m \u001b[0mbigram_probability\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Click here,  ..to win an iphone 11 pro max'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[0mbigram_probability\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Win a brand new car '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-c9fd4ff4cad7>\u001b[0m in \u001b[0;36mbigram_probability\u001b[1;34m(message)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mbigrams_for_msg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbigrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmatized_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Eliminamos stop words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mham_unigrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeature_extraction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocessing_msgs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mham_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mspam_unigrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeature_extraction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocessing_msgs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspam_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# Frecuencias de bigrams extraidas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocessing_msgs' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculando probabilidades del bigram\n",
    "def bigram_probability(message):\n",
    "    probability_h = 1\n",
    "    probability_s = 1\n",
    "    # Preprocesando los mensaje de entrada\n",
    "    punc_removed_message = \"\".join(word for word in message if word not in string.punctuation)\n",
    "    punc_removed_message = '<s> ' +punc_removed_message +' </s>'\n",
    "    tokenized_msg = re.split('\\s+', punc_removed_message)\n",
    "    lemmatized_msg = [word_lemmatizer.lemmatize(word)for word in tokenized_msg]\n",
    "    # bigrams para el mensaje\n",
    "    bigrams_for_msg = list(nltk.bigrams(lemmatized_msg))\n",
    "    # Eliminamos stop words\n",
    "    ham_unigrams = [word for word in feature_extraction(preprocessing_msgs(ham_text)) if word not in stopwords]\n",
    "    spam_unigrams = [word for word in feature_extraction(preprocessing_msgs(spam_text)) if word not in stopwords]\n",
    "    # Frecuencias de bigrams extraidas\n",
    "    ham_frequency = ham_bigram_feature_frequency()\n",
    "    spam_frequency  = spam_bigram_feature_frequency()\n",
    "    print('========================== Calculando Probabilidades ==========================')\n",
    "    print('----------- Frecuencias Ham ------------')\n",
    "    for bigram in bigrams_for_msg:\n",
    "        # probabilidad de la primera palabra en bigram\n",
    "        ham_probability_denominator = 0\n",
    "        # probabilidad de bigram (suavizado) \n",
    "        ham_probability_of_bigram = ham_frequency[bigram] + 1\n",
    "        print(bigram, ' ocurre ', ham_probability_of_bigram)\n",
    "        for (first_unigram, second_unigram) in filter_stopwords_bigrams(ham_unigrams):\n",
    "            ham_probability_denominator += 1\n",
    "            if(first_unigram == bigram[0]):\n",
    "                ham_probability_denominator += ham_frequency[first_unigram, second_unigram]\n",
    "        probability = ham_probability_of_bigram / ham_probability_denominator\n",
    "        probability_h *= probability\n",
    "    print('\\n')\n",
    "    print('----------- Frecuencias Spam ------------')\n",
    "    for bigram in bigrams_for_msg:\n",
    "        # probabilidad de la primera palabra en bigram\n",
    "        spam_probability_denominator = 0\n",
    "        # probabilidad de bigram (suavizado) \n",
    "        spam_probability_of_bigram = spam_frequency[bigram] + 1\n",
    "        print(bigram, ' ocurre ', spam_probability_of_bigram)\n",
    "        for (first_unigram, second_unigram) in filter_stopwords_bigrams(spam_unigrams):\n",
    "            spam_probability_denominator += 1\n",
    "            if(first_unigram == bigram[0]):\n",
    "                spam_probability_denominator += spam_frequency[first_unigram, second_unigram]\n",
    "        probability = spam_probability_of_bigram / spam_probability_denominator\n",
    "        probability_s *= probability\n",
    "    print('\\n')\n",
    "    print('Probabilidad Ham: ' +str(probability_h))\n",
    "    print('Probabildiad Spam: ' +str(probability_s))\n",
    "    print('\\n')\n",
    "    if(probability_h >= probability_s):\n",
    "        print('\\\"' +message +'\\\" es un mensaje Ham')\n",
    "    else:\n",
    "        print('\\\"' +message +'\\\" es un mensaje Spam')\n",
    "    print('\\n')\n",
    "bigram_probability('Click here,  ..to win an iphone 11 pro max')\n",
    "bigram_probability('Win a brand new car ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
